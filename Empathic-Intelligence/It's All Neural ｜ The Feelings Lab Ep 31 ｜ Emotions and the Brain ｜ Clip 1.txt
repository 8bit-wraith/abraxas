What is physically happening in our brain when we experience emotion?
What does that look like?
What is that?
So that's the question that we were trying to ask.
And the way that people had approached this problem before is they had basically picked
a few different stimuli, one or two or three per emotion.
And they posited, well, there's six emotions.
Let's see if we can distinguish these six emotions by essentially modeling which image
somebody was looking at or which video.
But there's so many different things.
Let's scare them and see what happens.
Let's make them laugh and see what happens.
Is that kind of thing?
Like show them something scary and then observe?
Is that what we were trying to do?
Essentially, that's the intuition, but the problem is that the things that they showed
participants differ in a litany of ways besides the fact that they evoke different emotions.
And so you look across many different studies that used those early techniques and you see
very inconsistent results.
And some of those inconsistencies have been interpreted by people who occupy kind of the
psychological constructivist camp that emotions are learned representations and not something
that you're born with.
Some of those studies have been interpreted as showing that emotions don't actually involve
any consistency in brain activity.
The brain is just kind of, yeah, each emotion corresponds to potentially many different
patterns that wouldn't be consistent across subjects or even necessarily across different
instances of somebody experiencing an emotion.
That was the claim.
So what we decided to do, what Yuki decided to do independently actually, was take that
approach and instead of just showing one or two stimuli per emotion, show as many stimuli
as possible so that you can actually try to separate out what kinds of patterns of brain
activity are evoked by the emotions somebody's experiencing in response to a stimulus versus
the perception of specific visual properties in the stimulus and semantic properties, like
are there people in it, are there animals?
And so we actually, with enough data, we can start to model that.
And so that's what we did.
That's what Yuki's lab is really good at.
And we have all this data on all these videos, what's in the video, what we can model the
visual components of the video.
And we found that once you do that, once you take care of all of the noise and the consistencies,
you find much more consistent representations of emotion that are not restricted to a few
different emotions that actually correspond to a really wide range of different emotions
over a dozen, at least.
And we tested 27 in total and they seem to all be decodable.
And the patterns of activity that correspond to the different emotions are consistent across
different people, remarkably consistent.
And they're more consistent than you could explain if you're trying to use kind of a
low dimensional theory of emotion.
For example, the psychological constructivist perspective is that things like valence, how
positive or negative something is, and arousal, how calm or excited it makes you, are actually
the core components of emotion and other things are sort of made up by the brain, constructed
by the brain in a way that's learned and not innate at all.
But in fact, what we found was that there's less consistency across participants and how
people represent or how different kinds of levels of valence, for example, are represented
in different brain regions or the same brain regions.
There's less consistency in that than there is for specific emotions, which is pretty remarkable.
