So, to what extent do different people express emotions in different ways, and can AI assist
in translating these expressions across different people?
Yeah, absolutely.
You see across cultures, there are differences in how people express emotion.
In East Asian cultures, people don't express many high arousal emotions, it's not a cultural
norm.
So generally, you'll conceal it if you're laughing sometimes, I mean, I'm not going to
too broadly generalize, but there is that trend.
And it's significant enough that in our data, we see, you know, half as much facial movement
in East Asian cultures in response to popular videos and social interactions versus in Western
cultures overall.
And so that's a huge difference, but people understand the same facial expressions, right?
People are able to see the facial expression, know what it means, you can normalize across
cultures.
And, you know, it's really fascinating actually about, I don't know if the audience has seen
it yet, but we have Arjun instead of Avatar, or Merzions Avatar playing simultaneously.
What's brilliant about this simulation is that, you know, at the same time, you probably
don't want to be talking to a real person in these training contexts, because then there's
a real person there and they have the presence of a person and you're being judged by a person.
This makes you feel like it's a playground, but this avatar is actually incredibly great
at listening.
It maintains all of the listening gestures, right?
Sort of the head nods and the looks of confusion and very like subtle eye twitches that people
use to express interest and maybe a little bit of doubt or confusion just at the right
times and really brings you in.
And I think that really embodies sort of how people engage with each other and with empathy
in very subtle ways that, you know, might otherwise be lost.
