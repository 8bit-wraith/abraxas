But like why a majority of tech companies are driven by the negative engagement and the
clicks at any cost instead of optimizing for a more positive user experience.
Any insight as to why we've ended up here?
Is it as simple as greed driven or is it more complex than that?
I mean, I think it's more complex when, you know, Jack and I have both been involved
inside the tech companies and having some of these conversations.
And the general, you know, it's like heists is that people want to fix this problem.
They don't want to optimize for engagement at all costs.
There's an excitement about the potential of optimizing more directly for measure indicators
of people's well-being.
I think the problem is that there's a worry about whether there's a scientific way to
do this that, you know, doesn't overly compress the idea of well-being into a single measure.
Often the question gets raised about what happens if you just make people smile more
and just ask the algorithm to make people smile.
Won't it just show people cat videos and then you won't have intelligent conversations
anymore?
That typically is like...
Where do I just end up?
That's awesome.
I want in.
I think that there's some truth to that, right?
And so what's clear out of that conversation is you can't just optimize for one thing.
You need multiple measures and you need nuance and the ability to do that is just emerging.
I mean, the ability to get language and nonverbal expression and measures of bullying and harassment
and hate speech and other things.
And make sure that when you're optimizing for one, you're not doing it to the detriment
of others or optimized for multiple measures of well-being simultaneously.
I think we're just starting to get to that point.
And so I'm optimistic that this will happen.
But in the meantime, because it's been difficult to come up with a measure we can agree on,
the default is what are users actually using, what's in that engagement and that can help
pros and cons.
