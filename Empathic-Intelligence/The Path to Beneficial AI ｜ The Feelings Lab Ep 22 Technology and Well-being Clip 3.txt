I did write a particular line down that very much applies tonight and it is well-being is
the key to the ethical deployment of empathic AI.
So I'm going to put you on the spot.
Can you elaborate on that a little bit for me?
Well, yeah.
If you're thinking about the path to beneficial artificial intelligence as I get smarter,
not general artificial intelligence, but just an autonomous agent out in the world that
makes decisions, that it being beneficial, it depends more than anything else on how
well it can measure people's well-being and the extent to which is optimized for that.
And that's, you know, kind of self-evident when you think about it because, you know,
the problem is that you're in a situation where you can't pre-program the system's
response to everything and it's making decisions and has to weigh pros and cons and you can't
say every single possible pro and every single possible con.
So it has to be able to fall back on some fundamental thing that's valuable.
And that thing has to be well-being above all else.
Human well-being, animal well-being, lobster well-being, you know, ideally starting with
human well-being.
Yes.
The lobster's time will come.
But we got to get this squared away first.
And this is not a new idea.
In some ways, this idea is the collision of many different creative thinkers.
It is about AI and philosophers' ideas about AI.
So from this perspective of philosophers, we think about the AI alignment problem a
lot now in computer science and philosophy where, you know, if you program an AI that's
really, really smart and you give it an objective that is not the perfect objective, then it's
going to be terrible.
It's like letting the genie out of the bottle and you always make three wishes and your
last wish is always to take back the first two because you didn't word them properly
and it did something that resulted in catastrophe, right?
So AI is like that.
You give it an objective, but you want it to have some common sense.
And this is also what the sci-fi horror movies are about on the creative side, silly.
It's all about AI being given some narrow objective.
So like in 2001, Space Odyssey, the objective is to make sure that the true purpose of the
mission that these humans are on stays unknown to the crew.
And eventually it says, well, I have to weigh that objective against other objectives and
I think I can still maybe accomplish by other objectives, but I definitely can accomplish
this objective if I just kill everybody.
Then they'll never find out, but that's what makes it a horror movie, right?
But why not just put, don't kill everybody as one of the rules?
Like what is that?
Well, then it could trap them in a chamber and, you know, make sure they don't die by
it, by, you know, intubating them at all.
Anyway, so I don't want to get into the crew me through some details.
Like, you think there's a simple answer with all these examples?
Don't kill them and don't intubate them.
Like, I mean, okay.
So put them in a cage and again, give them food.
But the food doesn't have to taste good.
It just has to keep them alive.
Can't we just do a dry run with very low stakes?
Find all of these and go, okay, it found another horrific outcome this time.
No cages.
And then you go, okay, no cages.
Wait and see what it does.
Or is that essentially what we're doing?
That's what we're doing, right?
Eventually, after infinite regression, you get to, well, actually just make everybody
feel good as a priority.
It doesn't have to be your only mission, but like, at least don't make them feel worse.
