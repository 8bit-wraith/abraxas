When you decided to kind of like the technology was fascinated, but you wanted to put this
empathy side to it, or you wanted to kind of do a kind of human betterment side to it,
was there a moment where you're like, huh?
That came way later.
So when I was in fourth grade, that was.
It was during my PhD when I was working with Docker and we started consulting with companies
because they were reaching out to us thinking, you know, saying, hey, you're publishing
on human expression.
We want to do that.
We want to build technology that sort of understands people a little better.
And working with the people at these tech companies who are interested in that, most
of them had a very, had really good intent.
And a lot of them wanted to build technology that understood people's well-being and could
optimize people's well-being.
Literally, there was a well-being team, there was the protecting care team.
Things have sort of gotten reorganized, but it was, there was an active engagement with
the fact that the technology of the day was optimized for engagement in many ways that
were almost explicitly antithetical to mindfulness, like pull us in, make us distracted, make
us forget why we opened the app in the first place.
And then we, you know, try to get us to stay in the app.
And so, you know, that's where I was drawn into, how do we, how do we apply what we
know in psychology to prevent this, even though this is kind of the default for technology?
If you're just going to optimize for the signal that you have, which is people using
the technology, seemingly that's the right signal to optimize for, because you're building
a technology and you want people to use it, it has this negative consequence.
And so that's where I realized that psychology had a big role to play in the development
of AI.
